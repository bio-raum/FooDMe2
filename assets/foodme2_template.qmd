---
title: "FooDMe2"
subtitle: "Taxonomic profiling of metabarcoding data"
date: now
date-format: YYYY-MM-DD, HH:mm
title-block-banner: "#3F51B5"
title-block-banner-color: white
format:
    html:
        embed-resources: true
        self-contained-math: true
        toc: true
        toc-expand: 2
        toc-location: left
        theme: lumen
        other-links:
            - text: Documentation
              icon: lightbulb
              href: https://bio-raum.github.io/FooDMe2/
            - text: Online repository
              icon: github
              href: https://github.com/bio-raum/FooDMe2
execute:
    echo: false
jupyter: python3
---

Report generated automatically by bio-raum/FooDMe2. Please check out our [documentation](https://bio-raum.github.io/FooDMe2/latest/).

```{python}
#| label: imports

import os
import json
import yaml
import pandas as pd
import plotly.express as px
from IPython.display import Markdown, IFrame, display, HTML
from tabulate import tabulate
from itables import show
```

```{=HTML}
<!-- Force plotly to recompute layout if a new panel tab is active -->
<!-- adapted from https://stackoverflow.com/a/62572610/602276 -->
<script type="text/javascript" charset="utf-8">
$(document).on('shown.bs.tab', function (event) {
    console.log("Tab shown");
    var doc = $(".tab-pane.active .plotly-graph-div");
    for (var i = 0; i < doc.length; i++) {
        _Plotly.relayout(doc[i], {autosize: true});
    }
});
</script>
```

```{python}
#| label: collect-sample-reports

json_files = [pos_json for pos_json in os.listdir('.') if pos_json.endswith('.report.json')]
```

# Summary

```{python}
#| label: collect-summary

summary = []

for json_file in json_files:
    with open(json_file) as f:
        jdata = json.load(f)

    sample = jdata["sample"]
    insert_size = jdata["fastp"]["insert_size"]["peak"]
    reads_total = int(int(jdata["fastp"]["summary"]["before_filtering"]["total_reads"])/2)
    q30 = round(float(jdata["fastp"]["summary"]["before_filtering"]["q30_rate"]),2)*100

    # Track the sample status
    this_status = "pass"
    try:
        reads_passing = jdata["cutadapt"]["Reads passing filters"]
        reads_filtered = jdata["cutadapt"]["Filtered reads (uncategorized)"]
    except KeyError:
        reads_passing = 0
        reads_filtered = 0
        this_status = "fail"
    try:
        reads_after_clustering = jdata["clustering"]["passing"]
        reads_chimera = jdata["clustering"]["chimeras"]
    except KeyError:
        reads_after_clustering = 0
        reads_chimera = 0
        this_status = "fail"

    # Check discarded proportion for warnings - Disabled for now
    # simple check if at least 80% of reads and more than 10000 available for assignment
    # if this_status == "pass":
    #    if "composition" not in jdata.keys():
    #        this_status = "fail"
    #    if reads_after_clustering < 0.8 * reads_total or reads_after_clustering < 10000:
    #        this_status = "warn"

    # sample-level dictionary
    headers = [
        "Sample",
        "Status",
        "Reads total",
        "Reads Q30 (%)",
        "Insert size peak",
        "Reads passing",
        "Reads filtered",
        "Reads after clustering",
        "Chimeric reads"
        ]
    summary.append([
        sample,
        this_status,
        reads_total,
        q30,
        insert_size,
        f"{reads_passing} ({round((reads_passing/reads_total)*100, 2)}%)",
        f"{reads_filtered} ({round((reads_filtered/reads_total)*100, 2)}%)",
        f"{reads_after_clustering} ({round((reads_after_clustering/reads_total)*100, 2)}%)",
        f"{reads_chimera} ({round((reads_chimera/reads_total)*100, 2)}%)",
    ])

df = pd.DataFrame(summary, columns=headers)
```

::: {.column-screen-inset-right}
```{python}
#| label: tbl-summary

def color_status(val):
    if val == "pass":
        color = "green"
    elif val == "warn":
        color = "orange"
    elif val == "fail":
        color = "red"
    return f"background-color: {color}"

ttips = pd.DataFrame(
    {
        "Status": "The overall analysis status: pass: ok to use, warn: potential issues found, fail: most probably not usable",
        "Reads total": "The number of reads before any processing",
        "Reads Q30 (%)": "Fraction of input reads >= Q30",
        "Insert size peak": "The peak insert size",
        "Reads passing": "The number of reads passing the primer and quality trimming",
        "Reads filtered": "The number of reads not passing the primer and quality trimming",
        "Reads after clustering": "The number of reads remaining after clustering",
        "Chimeric reads": "Reads classified as chimera during the clustering"
    },
    index=df.index
)

s = df.style.map(
    color_status, subset=pd.IndexSlice[:, ["Status"]]
).set_tooltips(
    ttips
).format(
    {"Reads Q30 (%)": "{:.2f}"}
)

show(
    s,
    classes="display compact",
    scrollY="600px",
    scrollCollapse=True,
    paging=False,
    buttons=["copyHtml5", "csvHtml5", "excelHtml5"]
)
```
:::

# Taxonomic composition

::: {.column-screen-inset-right}
```{python}
#| label: krona

krona = [fi for fi in os.listdir('.') if fi.endswith('_krona.html')]
IFrame(src=krona[0], width="100%", height="500px")
```
:::

# Input quality checking

Overview of input data quality before any processing.

## Insert size distribution

Insert size estimation of sampled reads.

::: {.column-screen-inset-right}
```{python}
#| label: insert-size

frames = []

for json_file in json_files:
    with open(json_file) as f:
        jdata = json.load(f)
    sample = jdata["sample"]
    try:
        # catch missing data if sample stopped
        counts = jdata["fastp"]["insert_size"]["histogram"]
    except KeyError:
        sdf = pd.DataFrame(columns=["sample", "size", "count"])
        frames.append(sdf)
        continue
    sdf = pd.DataFrame(
            list(zip([sample]*len(counts), range(1, len(counts)+1), counts)),
            columns=["sample", "size", "count"]
        )
    frames.append(sdf)

df = pd.concat(frames)

fig = px.line(df, x="size", y= "count", color="sample", hover_name="sample",
    labels={"size": "Insert size (bp)", "count": "Read count"},
    line_shape="spline", template="simple_white")
fig.update_traces(hovertemplate="%{y} reads")
fig.update_layout(hovermode="x unified")
fig.show()
```
:::

## Sequence Quality

Average sequencing quality over each base of all reads.

::: {.column-screen-inset-right}
```{python}
#| label: prep-read_quality

def get_qual(read_label):
    frames = []

    for json_file in json_files:
        with open(json_file) as f:
            jdata = json.load(f)
        sample = jdata["sample"]
        try:
            # catch missing data if sample stopped
            qual = jdata["fastp"][read_label]["quality_curves"]["mean"]
        except KeyError:
            sdf = pd.DataFrame(columns=["sample", "position", "qual"])
            frames.append(sdf)
            continue
        sdf = pd.DataFrame(
                list(zip([sample]*len(qual), range(1, len(qual)+1), qual)),
                columns=["sample", "position", "qual"]
            )
        frames.append(sdf)

    df = pd.concat(frames)
    return df
```
:::

::: {.panel-tabset .column-screen-inset-right}

### Read1: Before filtering

```{python}
#| label: read1_before

df = get_qual("read1_before_filtering")

fig = px.line(df, x="position", y= "qual", color="sample", hover_name="sample",
    labels={"position": "Read position", "qual": "Sequence quality"},
    line_shape="spline", template="simple_white")
fig.update_traces(hovertemplate="%{y}")
fig.update_layout(hovermode="x unified")
fig.show()
```

### Read1: After filtering

```{python}
#| label: read1_after

df = get_qual("read1_after_filtering")

fig = px.line(df, x="position", y= "qual", color="sample", hover_name="sample",
    labels={"position": "Read position", "qual": "Sequence quality"},
    line_shape="spline", template="simple_white")
fig.update_traces(hovertemplate="%{y}")
fig.update_layout(hovermode="x unified")
fig.show()
```

### Read2: Before filtering

```{python}
#| label: read2_before

df = get_qual("read2_before_filtering")

fig = px.line(df, x="position", y= "qual", color="sample", hover_name="sample",
    labels={"position": "Read position", "qual": "Sequence quality"},
    line_shape="spline", template="simple_white")
fig.update_traces(hovertemplate="%{y}")
fig.update_layout(hovermode="x unified")
fig.show()
```

### Read2: After filtering

```{python}
#| label: read2_after

df = get_qual("read2_after_filtering")

fig = px.line(df, x="position", y= "qual", color="sample", hover_name="sample",
    labels={"position": "Read position", "qual": "Sequence quality"},
    line_shape="spline", template="simple_white")
fig.update_traces(hovertemplate="%{y}")
fig.update_layout(hovermode="x unified")
fig.show()
```

:::

# Reads pre-processing

Removing low quality reads and removing primers.

## Filtered reads

Filtering low quality reads

::: {.panel-tabset group="countgraphs" .column-screen-inset-right}

### Counts

```{python}
#| label: filt-counts

frames = []

for json_file in json_files:
    with open(json_file) as f:
        jdata = json.load(f)
    sample = jdata["sample"]
    try:
        # catch missing data if sample stopped
        data = jdata["fastp"]["filtering_result"]
    except KeyError:
        sdf = pd.DataFrame(columns=["sample", "label", "count"])
        frames.append(sdf)
        continue
    labels, values = zip(*data.items())
    sdf = pd.DataFrame(
            list(zip([sample]*len(labels), labels, values)),
            columns=["sample", "label", "count"]
        )
    frames.append(sdf)

df = pd.concat(frames)

fig = px.bar(df, x="count", y= "sample", color="label", custom_data="label",
    labels={"count": "Read number", "sample": ""},
    orientation='h', template="simple_white")
fig.update_layout(legend={'title_text':''}, hovermode="y unified")
fig.update_yaxes(showspikes=False)
fig.update_traces(hovertemplate="%{customdata}: %{x}<extra></extra>")
fig.show()
```

### Percentages

```{python}
#| label: filt-perc

df["prop"] = 100*df["count"] / df.groupby("sample")["count"].transform("sum")

fig = px.bar(df, x="prop", y= "sample", color="label", custom_data="label",
    labels={"prop": "Read proportion (%)", "sample": ""},
    orientation='h', template="simple_white")
fig.update_layout(legend={'title_text':''}, hovermode="y unified")
fig.update_yaxes(showspikes=False)
fig.update_traces(hovertemplate="%{customdata}: %{x}%<extra></extra>")
fig.show()
```

:::

## Primer trimming

Find and remove primer sequences

::: {.panel-tabset group="countgraphs" .column-screen-inset-right}

### Counts

```{python}
#| label: trim-counts

frames = []

for json_file in json_files:
    with open(json_file) as f:
        jdata = json.load(f)
    sample = jdata["sample"]
    try:
        # catch missing data if sample stopped
        data = jdata["cutadapt"]
    except KeyError:
        sdf = pd.DataFrame(columns=["sample", "label", "count"])
        frames.append(sdf)
        continue
    labels, values = zip(*data.items())
    sdf = pd.DataFrame(
            list(zip([sample]*len(labels), labels, values)),
            columns=["sample", "label", "count"]
        )
    frames.append(sdf)

df = pd.concat(frames)

fig = px.bar(df, x="count", y= "sample", color="label", custom_data="label",
    labels={"count": "Read number", "sample": ""},
    orientation='h', template="simple_white")
fig.update_layout(legend={'title_text':''}, hovermode="y unified")
fig.update_yaxes(showspikes=False)
fig.update_traces(hovertemplate="%{customdata}: %{x}<extra></extra>")
fig.show()
```

### Percentages

```{python}
#| label: trim-perc

df["prop"] = 100*df["count"] / df.groupby("sample")["count"].transform("sum")

fig = px.bar(df, x="prop", y= "sample", color="label", custom_data="label",
    labels={"prop": "Read proportion (%)", "sample": ""},
    orientation='h', template="simple_white")
fig.update_layout(legend={'title_text':''}, hovermode="y unified")
fig.update_yaxes(showspikes=False)
fig.update_traces(hovertemplate="%{customdata}: %{x}%<extra></extra>")
fig.show()
```

:::

# Clustering

Clustering amplicon sequences

::: {.panel-tabset group="countgraphs" .column-screen-inset-right}

### Counts

```{python}
#| label: clus-counts

frames = []

for json_file in json_files:
    with open(json_file) as f:
        jdata = json.load(f)
    sample = jdata["sample"]
    try:
        # catch missing data if sample stopped
        data = jdata["clustering"]
    except KeyError:
        sdf = pd.DataFrame(columns=["sample", "label", "count"])
        frames.append(sdf)
        continue
    labels, values = zip(*data.items())
    sdf = pd.DataFrame(
            list(zip([sample]*len(labels), labels, values)),
            columns=["sample", "label", "count"]
        )
    frames.append(sdf)

df = pd.concat(frames)

fig = px.bar(df, x="count", y= "sample", color="label", custom_data="label",
    labels={"count": "Read number", "sample": ""},
    orientation='h', template="simple_white")
fig.update_layout(legend={'title_text':''}, hovermode="y unified")
fig.update_yaxes(showspikes=False)
fig.update_traces(hovertemplate="%{customdata}:%{x} reads<extra></extra>")
fig.show()
```

### Percentages

```{python}
#| label: clus-perc

df["prop"] = 100*df["count"] / df.groupby("sample")["count"].transform("sum")

fig = px.bar(df, x="prop", y= "sample", color="label", custom_data="label",
    labels={"prop": "Read proportion (%)", "sample": ""},
    orientation='h', template="simple_white")
fig.update_layout(legend={'title_text':''}, hovermode="y unified")
fig.update_yaxes(showspikes=False)
fig.update_traces(hovertemplate="%{customdata}: %{x}%<extra></extra>")
fig.show()
```

:::

# Taxonomic assignment

Taxonomic assignment of cluster sequences

::: {.panel-tabset group="countgraphs" .column-screen-inset-right}

### Counts

```{python}
#| label: ass-count

frames = []

for json_file in json_files:
    with open(json_file) as f:
        jdata = json.load(f)
    sample = jdata["sample"]
    try:
        # catch missing data if sample stopped
        data = jdata["composition"]
    except KeyError:
        sdf = pd.DataFrame(columns=["sample", "name", "taxid", "reads", "rank", "proportion"])
        frames.append(sdf)
        continue
    sdf = pd.DataFrame(
            data,
        )
    sdf["sample"] = sample
    frames.append(sdf)

df = pd.concat(frames)
df = df. groupby(["sample", "rank"])[["reads", "proportion"]].sum().reset_index()
df["proportion"] = df["proportion"]*100

fig = px.bar(df, x="reads", y= "sample", color="rank", custom_data="rank",
    labels={"reads": "Read number", "sample": ""},
    orientation='h', template="simple_white")
fig.update_layout(legend={'title_text':''}, hovermode="y unified")
fig.update_yaxes(showspikes=False)
fig.update_traces(hovertemplate="%{customdata}: %{x} reads<extra></extra>")
fig.show()
```

### Percentages

```{python}
#| label: ass-perc

fig = px.bar(df, x="proportion", y= "sample", color="rank", custom_data="rank",
    labels={"prop": "Read proportion (%)", "sample": ""},
    orientation='h', template="simple_white")
fig.update_layout(legend={'title_text':''}, hovermode="y unified")
fig.update_yaxes(showspikes=False)
fig.update_traces(hovertemplate="%{customdata}: %{x}%<extra></extra>")
fig.show()
```

:::

# Software versions

Collected at run time from the software output.

```{python}
#| label: versions

full = pd.DataFrame()

for json_file in json_files:
    with open(json_file) as f:
        jdata = json.load(f)
    sample = jdata["sample"]
    data = jdata["versions"]
    df_data = pd.json_normalize(data)
    index = pd.MultiIndex.from_arrays(zip(*[n.split(".") for n in df_data.columns]))
    df = pd.DataFrame(df_data.loc[0].values, index=index, columns=["versions"])
    if not full.empty:
        full = pd.merge(full, df, how='outer', left_index=True, right_index=True)
        # use max to fill NaN
        full = full.apply(max, axis=1).to_frame()
    else:
        full = df

full = full.reset_index().set_axis(["Module", "Software", "Version"], axis=1)
show(
    full,
    classes="display compact",
    scrollY="600px",
    scrollCollapse=True,
    paging=False,
    buttons=["copyHtml5", "csvHtml5"]
)
```
