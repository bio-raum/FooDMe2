1---
title: "FooDMe2"
subtitle: "Taxonomic profiling of metabarcoding data"
date: now
date-format: YYYY-MM-DD, HH:mm
format:
    html:
        embed-resources: true
        self-contained-math: true
        toc: true
        toc-expand: 2
        toc-location: left
        theme: lumen
        max-width: 2000px
execute:
    echo: false
jupyter: python3
---

Report generated automatically by bio-raum/FooDMe2. Please check out our [documentation](https://bio-raum.github.io/FooDMe2/latest/).

```{python}
#| label: imports

import os
import json
import yaml
import pandas as pd
import plotly.express as px
from IPython.display import Markdown, IFrame
from tabulate import tabulate
```

```{python}
#| label: collect-sample-reports

json_files = [pos_json for pos_json in os.listdir('.') if pos_json.endswith('.report.json')]
```

# Summary

```{python}
#| label: collect-summary

summary = []

for json_file in json_files:
    with open(json_file) as f:
        jdata = json.load(f)

    sample = jdata["sample"]
    insert_size = jdata["fastp"]["insert_size"]["peak"]
    reads_total = int(int(jdata["fastp"]["summary"]["before_filtering"]["total_reads"])/2)
    q30 = round(float(jdata["fastp"]["summary"]["before_filtering"]["q30_rate"]),2)*100

    # Track the sample status
    this_status = "pass"
    try:
        reads_passing = jdata["cutadapt"]["Reads passing filters"]
        reads_filtered = jdata["cutadapt"]["Filtered reads (uncategorized)"]
    except KeyError:
        reads_passing = 0
        reads_filtered = 0
        this_status = "fail"
    try:
        reads_after_clustering = jdata["clustering"]["passing"]
        reads_chimera = jdata["clustering"]["chimeras"]
    except KeyError:
        reads_after_clustering = 0
        reads_chimera = 0
        this_status = "fail"

    # sample-level dictionary
    headers = [
        "Sample",
        "Status",
        "Reads total",
        "Reads Q30 (%)",
        "Insert size peak",
        "Reads passing",
        "Reads filtered",
        "Reads after clustering",
        "Chimeric reads"
        ]
    summary.append([
        sample,
        this_status,
        reads_total,
        q30,
        insert_size,
        f"{reads_passing} ({round((reads_passing/reads_total)*100, 2)}%)",
        f"{reads_filtered} ({round((reads_filtered/reads_total)*100, 2)}%)",
        f"{reads_after_clustering} ({round((reads_after_clustering/reads_total)*100, 2)}%)",
        f"{reads_chimera} ({round((reads_chimera/reads_total)*100, 2)}%)",
    ])
```

```{python}
#| label: tbl-summary

Markdown(tabulate(
  summary,
  headers=headers
))
```

# Taxonomic composition

```{python}
#| label: krona

krona = [fi for fi in os.listdir('.') if fi.endswith('_krona.html')]
IFrame(src=krona[0], width="100%", height="500px")
```

# Input quality checking

Overview of input data quality before any processing.

## Insert size distribution

Insert size estimation of sampled reads.

```{python}
#| label: insert-size

frames = []

for json_file in json_files:
    with open(json_file) as f:
        jdata = json.load(f)
    sample = jdata["sample"]
    try:
        # catch missing data if sample stopped
        counts = jdata["fastp"]["insert_size"]["histogram"]
    except KeyError:
        sdf = pd.DataFrame(columns=["sample", "size", "count"])
        frames.append(sdf)
        continue
    sdf = pd.DataFrame(
            list(zip([sample]*len(counts), range(1, len(counts)+1), counts)),
            columns=["sample", "size", "count"]
        )
    frames.append(sdf)

df = pd.concat(frames)

fig = px.line(df, x="size", y= "count", color="sample", hover_name="sample",
    labels={"size": "Insert size (bp)", "count": "Read count"},
    line_shape="spline", template="simple_white")
fig.show()
```

## Sequence Quality

Average sequencing quality over each base of all reads.

```{python}
#| label: prep-read_quality

def get_qual(read_label):
    frames = []

    for json_file in json_files:
        with open(json_file) as f:
            jdata = json.load(f)
        sample = jdata["sample"]
        try:
            # catch missing data if sample stopped
            qual = jdata["fastp"][read_label]["quality_curves"]["mean"]
        except KeyError:
            sdf = pd.DataFrame(columns=["sample", "position", "qual"])
            frames.append(sdf)
            continue
        sdf = pd.DataFrame(
                list(zip([sample]*len(qual), range(1, len(qual)+1), qual)),
                columns=["sample", "position", "qual"]
            )
        frames.append(sdf)

    df = pd.concat(frames)
    return df
```

::: {.panel-tabset}

### Read1: Before filtering

```{python}
#| label: read1_before

df = get_qual("read1_before_filtering")

fig = px.line(df, x="position", y= "qual", color="sample", hover_name="sample",
    labels={"position": "Read position", "qual": "Sequence quality"},
    line_shape="spline", template="simple_white")
fig.show()
```

### Read1: After filtering

```{python}
#| label: read1_after

df = get_qual("read1_after_filtering")

fig = px.line(df, x="position", y= "qual", color="sample", hover_name="sample",
    labels={"position": "Read position", "qual": "Sequence quality"},
    line_shape="spline", template="simple_white")
fig.show()
```

### Read2: Before filtering

```{python}
#| label: read2_before

df = get_qual("read2_before_filtering")

fig = px.line(df, x="position", y= "qual", color="sample", hover_name="sample",
    labels={"position": "Read position", "qual": "Sequence quality"},
    line_shape="spline", template="simple_white")
fig.show()
```

### Read2: After filtering

```{python}
#| label: read2_after

df = get_qual("read2_after_filtering")

fig = px.line(df, x="position", y= "qual", color="sample", hover_name="sample",
    labels={"position": "Read position", "qual": "Sequence quality"},
    line_shape="spline", template="simple_white")
fig.show()
```

:::

# Reads pre-processing

Removing low quality reads and removing primers.

## Filtered reads

Filtering low quality reads

::: {.panel-tabset}

### Counts

```{python}
#| label: filt-counts

frames = []

for json_file in json_files:
    with open(json_file) as f:
        jdata = json.load(f)
    sample = jdata["sample"]
    try:
        # catch missing data if sample stopped
        data = jdata["fastp"]["filtering_result"]
    except KeyError:
        sdf = pd.DataFrame(columns=["sample", "label", "count"])
        frames.append(sdf)
        continue
    labels, values = zip(*data.items())
    sdf = pd.DataFrame(
            list(zip([sample]*len(labels), labels, values)),
            columns=["sample", "label", "count"]
        )
    frames.append(sdf)

df = pd.concat(frames)

fig = px.bar(df, x="count", y= "sample", color="label", hover_name="sample",
    labels={"count": "Read number", "sample": ""},
    orientation='h', template="simple_white")
fig.update_layout(legend={'title_text':''})
fig.show()
```

### Percentages

```{python}
#| label: filt-perc
pd.concat([
    df[["sample", "label"]],
    df.groupby("sample")[["count"]].transform(lambda x: (x/sum(x))*100)
], axis = 1)

fig = px.bar(df, x="count", y= "sample", color="label", hover_name="sample",
    labels={"count": "Read proportion (%)", "sample": ""},
    orientation='h', template="simple_white")
fig.update_layout(legend={'title_text':''})
fig.show()
```

:::

## Primer trimming

Find and remove primer sequences

::: {.panel-tabset}

### Counts

### Percentages

:::

# Clustering

Clustering amplicon sequences

::: {.panel-tabset}

### Counts

### Percentages

:::

# Taxonomic assignment

Taxonomic assignment of cluster sequences

# Software versions

Collected at run time from the software output.

TBD: collect versions over all sample reports and produce a table like for summary