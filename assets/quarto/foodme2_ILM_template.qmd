---
title: "FooDMe2"
subtitle: "Taxonomic profiling of metabarcoding data"
date: now
date-format: YYYY-MM-DD, HH:mm
title-block-banner: "#3F51B5"
title-block-banner-color: white
format:
    html:
        embed-resources: true
        self-contained-math: true
        toc: true
        toc-expand: 2
        toc-location: left
        theme: lumen
        other-links:
            - text: Documentation
              icon: lightbulb
              href: https://bio-raum.github.io/FooDMe2/
            - text: Online repository
              icon: github
              href: https://github.com/bio-raum/FooDMe2
execute:
    echo: false
jupyter: python3
---

Report generated automatically by bio-raum/FooDMe2. Please check out our [documentation](https://bio-raum.github.io/FooDMe2/latest/).

```{python}
#| label: imports

import os
import json
import yaml
import pandas as pd
import plotly.express as px
from IPython.display import Markdown, IFrame, display, HTML
from tabulate import tabulate
from itables import show
```

```{=HTML}
<!-- Force plotly to recompute layout if a new panel tab is active -->
<!-- adapted from https://stackoverflow.com/a/62572610/602276 -->
<script type="text/javascript" charset="utf-8">
$(document).on('shown.bs.tab', function (event) {
    console.log("Tab shown");
    var doc = $(".tab-pane.active .plotly-graph-div");
    for (var i = 0; i < doc.length; i++) {
        _Plotly.relayout(doc[i], {autosize: true});
    }
});
</script>
```

```{python}
#| label: collect-sample-reports

json_files = [pos_json for pos_json in os.listdir('.') if pos_json.endswith('.summary.json')]

```

# Summary

```{python}
#| label: collect-summary

summary = []

for json_file in json_files:
    with open(json_file) as f:
        jdata = json.load(f)

    sample = jdata["sample"]
    
    # check if PE or SE
    if "read2_before_filtering" in jdata["fastp"]:
        insert_size = jdata["fastp"]["insert_size"]["peak"]
        read_config = 2
    else:
        insert_size = jdata["fastp"]["summary"]["before_filtering"]["read1_mean_length"]
        read_config = 1
    
    reads_total = int(int(jdata["fastp"]["summary"]["before_filtering"]["total_reads"])/read_config)
    q30 = round(float(jdata["fastp"]["summary"]["before_filtering"]["q30_rate"]),2)*100

    # Track the sample status
    this_status = "pass"
    try:
        reads_passing = int(int(jdata["fastp_trimmed"]["summary"]["after_filtering"]["total_reads"])/read_config)
        # Count all filtered reads, i.e. primer trimming + quality
        reads_filtered = reads_total - reads_passing
    except KeyError:
        reads_passing = 0
        reads_filtered = 0
        this_status = "fail"
    try:
        reads_after_clustering = int(jdata["clustering"]["passing"])
        reads_chimera = int(jdata["clustering"]["chimeras"])
    except KeyError:
        reads_after_clustering = 0
        reads_chimera = 0
        this_status = "fail"

    # Check discarded proportion for warnings and fails
    if this_status == "pass":
        if "composition" not in jdata.keys() or reads_after_clustering < 500:
           this_status = "fail"
        elif reads_passing < 0.9 * reads_total or reads_after_clustering < 0.9 * reads_passing:
           this_status = "warn"

    headers = [
        "Sample",
        "Status",
        "Reads total",
        "Reads Q30 (%)",
        "Insert size peak",
        "Reads passing",
        "Reads filtered",
        "Reads after clustering",
        "Chimeric reads"
        ]

    try:
        summary.append([
            sample,
            this_status,
            reads_total,
            q30,
            insert_size,
            f"{reads_passing} ({round((reads_passing/reads_total)*100, 2)}%)",
            f"{reads_filtered} ({round((reads_filtered/reads_total)*100, 2)}%)",
            f"{reads_after_clustering} ({round((reads_after_clustering/reads_total)*100, 2)}%)",
            f"{reads_chimera} ({round((reads_chimera/reads_total)*100, 2)}%)",
        ])
    except ZeroDivisionError:
        summary.append([
            sample,
            this_status,
            reads_total,
            q30,
            insert_size,
            f"{reads_passing}",
            f"{reads_filtered}",
            f"{reads_after_clustering}",
            f"{reads_chimera}",
        ])

df = pd.DataFrame(summary, columns=headers).sort_values(by=["Sample"], ignore_index=True)
if read_config == 1:
    df = df.rename(columns={"Insert size peak": "Read length"})
```

::: {.column-screen-inset-right}
```{python}
#| label: tbl-summary

def color_status(val):
    if val == "pass":
        color = "limegreen"
    elif val == "warn":
        color = "orange"
    elif val == "fail":
        color = "lightcoral"
    return f"background-color: {color}"

if read_config == 1:
    insert_name = "Read length"
    insert_ttip = "The mean of read lengths"
else:
    insert_name = "Insert size peak"
    insert_ttip = "The peak insert size"

ttips = pd.DataFrame(
    {
        "Status": "The overall analysis status: pass: ok to use, warn: potential issues found, fail: most probably not usable",
        "Reads total": "The number of reads before any processing",
        "Reads Q30 (%)": "Fraction of input reads >= Q30",
        insert_name: insert_ttip,
        "Reads passing": "The number of reads passing the primer and quality trimming",
        "Reads filtered": "The number of reads not passing the primer and quality trimming",
        "Reads after clustering": "The number of reads remaining after clustering",
        "Chimeric reads": "Reads classified as chimera during the clustering"
    },
    index=df.index
)

s = df.style.map(
    color_status, subset=pd.IndexSlice[:, ["Status"]]
).set_tooltips(
    ttips
).format(
    {"Reads Q30 (%)": "{:.2f}"}
)

show(
    s,
    classes="display compact",
    scrollY="600px",
    scrollCollapse=True,
    paging=False,
    buttons=["copyHtml5", "csvHtml5", "excelHtml5"]
)
```
:::

# Taxonomic composition

::: {.column-screen-inset-right}
```{python}
#| label: krona

krona = [fi for fi in os.listdir('.') if fi.endswith('_krona.html')]
IFrame(src=krona[0], width="100%", height="500px")
```
:::

# Input quality checking

Overview of input data quality before any processing.

```{python}
def get_insert_hist(fastp_label):
    frames = []

    for json_file in json_files:
        with open(json_file) as f:
            jdata = json.load(f)
        sample = jdata["sample"]
        try:
            # catch missing data if sample stopped
            counts = jdata[fastp_label]["insert_size"]["histogram"]
        except KeyError:
            sdf = pd.DataFrame(columns=["sample", "size", "count"])
            frames.append(sdf)
            continue
        sdf = pd.DataFrame(
                list(zip([sample]*len(counts), range(1, len(counts)+1), counts)),
                columns=["sample", "size", "count"]
            )
        frames.append(sdf)

    df = pd.concat([frame for frame in frames if not frame.empty])
    return df
```


## Insert size distribution

Insert size estimation of sampled reads.

```{python}
#| output: asis
# Skip this if single end
if read_config == 1:
    print("Not possible for single-end sequencing data.")
```

::: {.panel-tabset .column-screen-inset-right}

```{python}
#| output: asis
# Skip this if single end
if read_config == 2:
    print("### Before trimming")
```

```{python}
if read_config == 2:
    df = get_insert_hist("fastp")

    fig = px.line(df, x="size", y= "count", color="sample", hover_name="sample",
        labels={"size": "Insert size (bp)", "count": "Read count"},
        line_shape="spline", template="simple_white")
    fig.update_traces(hovertemplate="%{x}bp: %{y} reads")
    fig.update_layout(hovermode="closest")
    fig.show()
```

```{python}
#| output: asis
# Skip this if single end
if read_config == 2:
    print("### After trimming")
```

```{python}
if read_config == 2:
    df = get_insert_hist("fastp_trimmed")

    fig = px.line(df, x="size", y= "count", color="sample", hover_name="sample",
        labels={"size": "Insert size (bp)", "count": "Read count"},
        line_shape="spline", template="simple_white")
    fig.update_traces(hovertemplate="%{x}bp: %{y} reads")
    fig.update_layout(hovermode="closest")
    fig.show()
```

:::

## Sequence Quality

Average sequencing quality over each base of all reads.


```{python}
#| label: prep-read_quality

def get_qual(fastp_label, read_label):
    frames = []

    for json_file in json_files:
        with open(json_file) as f:
            jdata = json.load(f)
        sample = jdata["sample"]
        try:
            # catch missing data if sample stopped
            qual = jdata[fastp_label][read_label]["quality_curves"]["mean"]
        except KeyError:
            sdf = pd.DataFrame(columns=["sample", "position", "qual"])
            frames.append(sdf)
            continue
        sdf = pd.DataFrame(
                list(zip([sample]*len(qual), range(1, len(qual)+1), qual)),
                columns=["sample", "position", "qual"]
            )
        frames.append(sdf)

    df = pd.concat([frame for frame in frames if not frame.empty])
    return df
```

::: {.panel-tabset .column-screen-inset-right}

### Read1: Before filtering

```{python}
#| label: read1_before

df = get_qual("fastp", "read1_before_filtering")

fig = px.line(df, x="position", y= "qual", color="sample", hover_name="sample",
    labels={"position": "Read position", "qual": "Sequence quality"},
    line_shape="spline", template="simple_white")
fig.update_traces(hovertemplate="#%{x}: %{y}")
fig.update_layout(hovermode="closest")
fig.show()
```

### Read1: After filtering

```{python}
#| label: read1_after

df = get_qual("fastp_trimmed", "read1_after_filtering")

fig = px.line(df, x="position", y= "qual", color="sample", hover_name="sample",
    labels={"position": "Read position", "qual": "Sequence quality"},
    line_shape="spline", template="simple_white")
fig.update_traces(hovertemplate="#%{x}: %{y}")
fig.update_layout(hovermode="closest")
fig.show()
```

```{python}
#| output: asis
# Skip this if single end
if read_config == 2:
    print("### Read2: Before filtering")
```

```{python}
# Skip this if single end
if read_config == 2:
    df = get_qual("fastp", "read2_before_filtering")

    fig = px.line(df, x="position", y= "qual", color="sample", hover_name="sample",
        labels={"position": "Read position", "qual": "Sequence quality"},
        line_shape="spline", template="simple_white")
    fig.update_traces(hovertemplate="#%{x}: %{y}")
    fig.update_layout(hovermode="closest")
    fig.show()
```

```{python}
#| output: asis
# Skip this if single end
if read_config == 2:
    print("### Read2: After filtering")
```

```{python}
# Skip this if single end
if read_config == 2:
    df = get_qual("fastp_trimmed", "read2_after_filtering")

    fig = px.line(df, x="position", y= "qual", color="sample", hover_name="sample",
        labels={"position": "Read position", "qual": "Sequence quality"},
        line_shape="spline", template="simple_white")
    fig.update_traces(hovertemplate="#%{x}: %{y}")
    fig.update_layout(hovermode="closest")
    fig.show()
```

:::

# Reads pre-processing

Removing low quality reads and removing primers.

## Primer trimming

Find and remove primer sequences

::: {.panel-tabset group="countgraphs" .column-screen-inset-right}

### Counts

```{python}
#| label: trim-counts

frames = []

for json_file in json_files:
    with open(json_file) as f:
        jdata = json.load(f)
    sample = jdata["sample"]
    try:
        # catch missing data if sample stopped
        data = jdata["cutadapt"]
    except KeyError:
        sdf = pd.DataFrame(columns=["sample", "label", "count"])
        frames.append(sdf)
        continue
    labels, values = zip(*data.items())
    sdf = pd.DataFrame(
            list(zip([sample]*len(labels), labels, values)),
            columns=["sample", "label", "count"]
        )
    frames.append(sdf)

df = pd.concat([frame for frame in frames if not frame.empty])

fig = px.bar(df, x="count", y= "sample", color="label", custom_data="label",
    labels={"count": "Read number", "sample": ""},
    orientation='h', template="simple_white")
fig.update_layout(legend={'title_text':''}, hovermode="y unified")
fig.update_yaxes(showspikes=False)
fig.update_traces(hovertemplate="%{customdata}: %{x}<extra></extra>")
fig.show()
```

### Percentages

```{python}
#| label: trim-perc

df["prop"] = 100*df["count"] / df.groupby("sample")["count"].transform("sum")

fig = px.bar(df, x="prop", y= "sample", color="label", custom_data="label",
    labels={"prop": "Read proportion (%)", "sample": ""},
    orientation='h', template="simple_white")
fig.update_layout(legend={'title_text':''}, hovermode="y unified")
fig.update_yaxes(showspikes=False)
fig.update_traces(hovertemplate="%{customdata}: %{x}%<extra></extra>")
fig.show()
```

:::

## Filtered reads

Filtering low quality reads

::: {.panel-tabset group="countgraphs" .column-screen-inset-right}

### Counts

```{python}
#| label: filt-counts

frames = []

for json_file in json_files:
    with open(json_file) as f:
        jdata = json.load(f)
    sample = jdata["sample"]
    try:
        # catch missing data if sample stopped
        data = jdata["fastp_trimmed"]["filtering_result"]
    except KeyError:
        sdf = pd.DataFrame(columns=["sample", "label", "count"])
        frames.append(sdf)
        continue
    labels, values = zip(*data.items())
    sdf = pd.DataFrame(
            list(zip([sample]*len(labels), labels, values)),
            columns=["sample", "label", "count"]
        )
    frames.append(sdf)

df = pd.concat([frame for frame in frames if not frame.empty])

fig = px.bar(df, x="count", y= "sample", color="label", custom_data="label",
    labels={"count": "Read number", "sample": ""},
    orientation='h', template="simple_white")
fig.update_layout(legend={'title_text':''}, hovermode="y unified")
fig.update_yaxes(showspikes=False)
fig.update_traces(hovertemplate="%{customdata}: %{x}<extra></extra>")
fig.show()
```

### Percentages

```{python}
#| label: filt-perc

df["prop"] = 100*df["count"] / df.groupby("sample")["count"].transform("sum")

fig = px.bar(df, x="prop", y= "sample", color="label", custom_data="label",
    labels={"prop": "Read proportion (%)", "sample": ""},
    orientation='h', template="simple_white")
fig.update_layout(legend={'title_text':''}, hovermode="y unified")
fig.update_yaxes(showspikes=False)
fig.update_traces(hovertemplate="%{customdata}: %{x}%<extra></extra>")
fig.show()
```

:::

# Clustering

Clustering amplicon sequences

::: {.panel-tabset group="countgraphs" .column-screen-inset-right}

### Counts

```{python}
#| label: clus-counts

frames = []

for json_file in json_files:
    with open(json_file) as f:
        jdata = json.load(f)
    sample = jdata["sample"]
    try:
        # catch missing data if sample stopped
        data = jdata["clustering"]
    except KeyError:
        sdf = pd.DataFrame(columns=["sample", "label", "count"])
        frames.append(sdf)
        continue
    labels, values = zip(*data.items())
    sdf = pd.DataFrame(
            list(zip([sample]*len(labels), labels, values)),
            columns=["sample", "label", "count"]
        )
    frames.append(sdf)

df = pd.concat([frame for frame in frames if not frame.empty])

fig = px.bar(df, x="count", y= "sample", color="label", custom_data="label",
    labels={"count": "Read number", "sample": ""},
    orientation='h', template="simple_white")
fig.update_layout(legend={'title_text':''}, hovermode="y unified")
fig.update_yaxes(showspikes=False)
fig.update_traces(hovertemplate="%{customdata}:%{x} reads<extra></extra>")
fig.show()
```

### Percentages

```{python}
#| label: clus-perc

df["prop"] = 100*df["count"] / df.groupby("sample")["count"].transform("sum")

fig = px.bar(df, x="prop", y= "sample", color="label", custom_data="label",
    labels={"prop": "Read proportion (%)", "sample": ""},
    orientation='h', template="simple_white")
fig.update_layout(legend={'title_text':''}, hovermode="y unified")
fig.update_yaxes(showspikes=False)
fig.update_traces(hovertemplate="%{customdata}: %{x}%<extra></extra>")
fig.show()
```

:::

# Taxonomic assignment

Taxonomic assignment of cluster sequences

::: {.panel-tabset group="countgraphs" .column-screen-inset-right}

### Counts

```{python}
#| label: ass-count

frames = []

for json_file in json_files:
    with open(json_file) as f:
        jdata = json.load(f)
    sample = jdata["sample"]
    try:
        # catch missing data if sample stopped
        data = jdata["composition"]
    except KeyError:
        sdf = pd.DataFrame(columns=["sample", "name", "taxid", "reads", "rank", "proportion"])
        frames.append(sdf)
        continue
    sdf = pd.DataFrame(
            data,
        )
    sdf["sample"] = sample
    frames.append(sdf)

df = pd.concat([frame for frame in frames if not frame.empty])
df = df. groupby(["sample", "rank"])[["reads", "proportion"]].sum().reset_index()
df["proportion"] = df["proportion"]*100

fig = px.bar(df, x="reads", y= "sample", color="rank", custom_data="rank",
    labels={"reads": "Read number", "sample": ""},
    orientation='h', template="simple_white")
fig.update_layout(legend={'title_text':''}, hovermode="y unified")
fig.update_yaxes(showspikes=False)
fig.update_traces(hovertemplate="%{customdata}: %{x} reads<extra></extra>")
fig.show()
```
9
### Percentages

```{python}
#| label: ass-perc

fig = px.bar(df, x="proportion", y= "sample", color="rank", custom_data="rank",
    labels={"prop": "Read proportion (%)", "sample": ""},
    orientation='h', template="simple_white")
fig.update_layout(legend={'title_text':''}, hovermode="y unified")
fig.update_yaxes(showspikes=False)
fig.update_traces(hovertemplate="%{customdata}: %{x}%<extra></extra>")
fig.show()
```

:::

# Software versions

Collected at run time from the software output.

```{python}
#| label: versions

full = pd.DataFrame()

for json_file in json_files:
    with open(json_file) as f:
        jdata = json.load(f)
    sample = jdata["sample"]
    data = jdata["versions"]
    df_data = pd.json_normalize(data)
    index = pd.MultiIndex.from_arrays(zip(*[n.split(".") for n in df_data.columns]))
    df = pd.DataFrame(df_data.loc[0].values, index=index, columns=["versions"])
    if not full.empty:
        full = pd.merge(full, df, how='outer', left_index=True, right_index=True)
        # use max to fill NaN
        full = full.apply(max, axis=1).to_frame()
    else:
        full = df

full = full.reset_index().set_axis(["Module", "Software", "Version"], axis=1)
show(
    full,
    classes="display compact",
    scrollY="600px",
    scrollCollapse=True,
    paging=False,
    buttons=["copyHtml5", "csvHtml5"]
)
```

:::

# Pipeline Settings

```{python}
#| label: settings

psettings = False

settings_file = [pos_json for pos_json in os.listdir('.') if pos_json.startswith('params_')].pop(0)

with open(settings_file) as f:
    psettings = json.load(f)

table = []

psettings.pop('maxMultiqcEmailFileSize', None)
psettings.pop('primers', None)
psettings.pop('references', None)

for key,value in (sorted(psettings.items())):
    table.append([key, value])

Markdown(tabulate(table, headers=["Setting","Value"]))

```
