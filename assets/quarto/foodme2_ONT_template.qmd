---
title: "FooDMe2"
subtitle: "Taxonomic profiling of metabarcoding data"
date: now
date-format: YYYY-MM-DD, HH:mm
title-block-banner: "#3F51B5"
title-block-banner-color: white
format:
    html:
        embed-resources: true
        self-contained-math: true
        toc: true
        toc-expand: 2
        toc-location: left
        theme: lumen
        other-links:
            - text: Documentation
              icon: lightbulb
              href: https://bio-raum.github.io/FooDMe2/
            - text: Online repository
              icon: github
              href: https://github.com/bio-raum/FooDMe2
execute:
    echo: false
jupyter: python3
---

Report generated automatically by bio-raum/FooDMe2. Please check out our [documentation](https://bio-raum.github.io/FooDMe2/latest/).

```{python}
#| label: imports

import os
import json
import yaml
import pandas as pd
import plotly.express as px
from IPython.display import Markdown, IFrame, display, HTML
from tabulate import tabulate
from itables import show
```

```{=HTML}
<!-- Force plotly to recompute layout if a new panel tab is active -->
<!-- adapted from https://stackoverflow.com/a/62572610/602276 -->
<script type="text/javascript" charset="utf-8">
$(document).on('shown.bs.tab', function (event) {
    console.log("Tab shown");
    var doc = $(".tab-pane.active .plotly-graph-div");
    for (var i = 0; i < doc.length; i++) {
        _Plotly.relayout(doc[i], {autosize: true});
    }
});
</script>
```

```{python}
#| label: collect-sample-reports

json_files = [pos_json for pos_json in os.listdir('.') if pos_json.endswith('.summary.json')]
```

# Summary

```{python}
#| label: collect-summary

summary = []
# sample-level dictionary
headers = [
    "Sample",
    "Status",
    "Reads total",
    "Reads Q20 (%)",
    "Mean insert size",
    "Reads passing",
    "Reads filtered",
    "Reads after clustering",
    "Chimeric reads"
    ]

for json_file in json_files:
    with open(json_file) as f:
        jdata = json.load(f)

    sample = jdata["sample"]
    insert_size = int(jdata["fastplong_trimmed"]["summary"]["after_filtering"]["read_mean_length"])
    reads_total = int(int(jdata["fastplong"]["summary"]["before_filtering"]["total_reads"])/2)
    q20 = round(float(jdata["fastplong"]["summary"]["before_filtering"]["q20_rate"]),2)*100
    reads_passing = "not implemented"
    reads_filtered = "not implemented"

    # Track the sample status
    this_status = "pass"
    try:
        reads_passing = int(jdata["fastplong_trimmed"]["summary"]["after_filtering"]["total_reads"])
        # Count all filtered reads, i.e. primer trimming + quality
        reads_filtered = reads_total - reads_passing
    except KeyError:
        reads_passing = 0
        reads_filtered = 0
        this_status = "fail"

    try:
        reads_after_clustering = int(jdata["clustering"]["passing"])
        reads_chimera = int(jdata["clustering"]["chimeras"])
    except KeyError:
        reads_after_clustering = 0
        reads_chimera = 0
        this_status = "fail"

    # Check discarded proportion for warnings and fails
    if this_status == "pass":
        if "composition" not in jdata.keys() or reads_after_clustering < 500:
           this_status = "fail"
        # elif reads_passing < 0.9 * reads_total or reads_after_clustering < 0.9 * reads_passing:
           # this_status = "warn"

    try:
        summary.append([
            sample,
            this_status,
            reads_total,
            q20,
            insert_size,
            reads_passing,
            reads_filtered,
            reads_after_clustering,
            reads_chimera,
        ])
    # Leaving this for later- See Illumina report
    except ZeroDivisionError:
        summary.append([
            sample,
            this_status,
            reads_total,
            q20,
            insert_size,
            f"{reads_passing}",
            f"{reads_filtered}",
            f"{reads_after_clustering}",
            f"{reads_chimera}",
        ])

df = pd.DataFrame(summary, columns=headers).sort_values(by=["Sample"], ignore_index=True)
```

::: {.column-screen-inset-right}
```{python}
#| label: tbl-summary

def color_status(val):
    if val == "pass":
        color = "limegreen"
    elif val == "warn":
        color = "orange"
    elif val == "fail":
        color = "lightcoral"
    return f"background-color: {color}"

ttips = pd.DataFrame(
    {
        "Status": "The overall analysis status: pass: ok to use, warn: potential issues found, fail: most probably not usable",
        "Reads total": "The number of reads before any processing",
        "Reads Q20 (%)": "Fraction of input reads >= Q20",
        "Insert size peak": "The peak insert size",
        "Reads passing": "The number of reads passing the primer and quality trimming",
        "Reads filtered": "The number of reads not passing the primer and quality trimming",
        "Reads after clustering": "The number of reads remaining after clustering",
        "Chimeric reads": "Reads classified as chimera during the clustering"
    },
    index=df.index
)

s = df.style.map(
    color_status, subset=pd.IndexSlice[:, ["Status"]]
).set_tooltips(
    ttips
# ).format(
    # {"Reads Q30 (%)": "{:.2f}"}
)

show(
    s,
    classes="display compact",
    scrollY="600px",
    scrollCollapse=True,
    paging=False,
    buttons=["copyHtml5", "csvHtml5", "excelHtml5"]
)
```
:::

# Taxonomic composition

::: {.column-screen-inset-right}
```{python}
#| label: krona

krona = [fi for fi in os.listdir('.') if fi.endswith('_krona.html')]
IFrame(src=krona[0], width="100%", height="500px")
```
:::

# Input quality checking

Overview of input data quality before any processing.

## Insert size distribution

Insert size estimation of sampled reads.

::: {.column-screen-inset-right} 
```{python}
#| label: prep_insert-size

def get_insert_hist(label):
    frames = []

    for json_file in json_files:
        with open(json_file) as f:
            jdata = json.load(f)
        sample = jdata["sample"]
        try:
            # catch missing data if sample stopped
            values = sorted(jdata[label]["lengths"])
            counts = []
            for x in range(1000):
                matches = len([v for v in values if v == x])
                counts.append(matches)
        except KeyError:
            sdf = pd.DataFrame(columns=["sample", "size", "count"])
            frames.append(sdf)
            continue
        sdf = pd.DataFrame(
                list(zip([sample]*len(counts), range(1, len(counts)+1), counts)),
                columns=["sample", "size", "count"]
            )
        frames.append(sdf)

    df = pd.concat([frame for frame in frames if not frame.empty])
    return df


#| label: insert_size_before
df = get_insert_hist("nanoplot")

fig = px.line(df, x="size", y= "count", color="sample", hover_name="sample",
    labels={"size": "Read lengths (bp)", "count": "Read count"},
    line_shape="spline", template="simple_white")
fig.update_traces(hovertemplate="%{x}bp: %{y} reads")
fig.update_layout(hovermode="closest")
fig.show()
```
:::

## Sequence Quality

Average sequencing quality over each base of all reads.

Not Implemented for ONT!

# Reads pre-processing

Removing low quality reads and removing primers.

## Primer trimming

Find and remove primer sequences

::: {.panel-tabset group="countgraphs" .column-screen-inset-right}

### Counts

```{python}
#| label: trim-counts

frames = []

for json_file in json_files:
    with open(json_file) as f:
        jdata = json.load(f)
    sample = jdata["sample"]
    try:
        # catch missing data if sample stopped
        data = jdata["cutadapt"]
    except KeyError:
        sdf = pd.DataFrame(columns=["sample", "label", "count"])
        frames.append(sdf)
        continue
    labels, values = zip(*data.items())
    sdf = pd.DataFrame(
            list(zip([sample]*len(labels), labels, values)),
            columns=["sample", "label", "count"]
        )
    frames.append(sdf)

df = pd.concat([frame for frame in frames if not frame.empty])

fig = px.bar(df, x="count", y= "sample", color="label", custom_data="label",
    labels={"count": "Read number", "sample": ""},
    orientation='h', template="simple_white")
fig.update_layout(legend={'title_text':''}, hovermode="y unified")
fig.update_yaxes(showspikes=False)
fig.update_traces(hovertemplate="%{customdata}: %{x}<extra></extra>")
fig.show()
```

### Percentages

```{python}
#| label: trim-perc

df["prop"] = 100*df["count"] / df.groupby("sample")["count"].transform("sum")

fig = px.bar(df, x="prop", y= "sample", color="label", custom_data="label",
    labels={"prop": "Read proportion (%)", "sample": ""},
    orientation='h', template="simple_white")
fig.update_layout(legend={'title_text':''}, hovermode="y unified")
fig.update_yaxes(showspikes=False)
fig.update_traces(hovertemplate="%{customdata}: %{x}%<extra></extra>")
fig.show()
```

:::

## Filtered reads

Filtering low quality reads

Not Implemented for ONT!

# Clustering

Clustering amplicon sequences

::: {.panel-tabset group="countgraphs" .column-screen-inset-right}

### Counts

```{python}
#| label: clus-counts

frames = []

for json_file in json_files:
    with open(json_file) as f:
        jdata = json.load(f)
    sample = jdata["sample"]
    try:
        # catch missing data if sample stopped
        data = jdata["clustering"]
    except KeyError:
        sdf = pd.DataFrame(columns=["sample", "label", "count"])
        frames.append(sdf)
        continue
    labels, values = zip(*data.items())
    sdf = pd.DataFrame(
            list(zip([sample]*len(labels), labels, values)),
            columns=["sample", "label", "count"]
        )
    frames.append(sdf)

df = pd.concat([frame for frame in frames if not frame.empty])

fig = px.bar(df, x="count", y= "sample", color="label", custom_data="label",
    labels={"count": "Read number", "sample": ""},
    orientation='h', template="simple_white")
fig.update_layout(legend={'title_text':''}, hovermode="y unified")
fig.update_yaxes(showspikes=False)
fig.update_traces(hovertemplate="%{customdata}:%{x} reads<extra></extra>")
fig.show()
```

### Percentages

```{python}
#| label: clus-perc

df["prop"] = 100*df["count"] / df.groupby("sample")["count"].transform("sum")

fig = px.bar(df, x="prop", y= "sample", color="label", custom_data="label",
    labels={"prop": "Read proportion (%)", "sample": ""},
    orientation='h', template="simple_white")
fig.update_layout(legend={'title_text':''}, hovermode="y unified")
fig.update_yaxes(showspikes=False)
fig.update_traces(hovertemplate="%{customdata}: %{x}%<extra></extra>")
fig.show()
```

:::

# Taxonomic assignment

Taxonomic assignment of cluster sequences

::: {.panel-tabset group="countgraphs" .column-screen-inset-right}

### Counts

```{python}
#| label: ass-count

frames = []

for json_file in json_files:
    with open(json_file) as f:
        jdata = json.load(f)
    sample = jdata["sample"]
    try:
        # catch missing data if sample stopped
        data = jdata["composition"]
    except KeyError:
        sdf = pd.DataFrame(columns=["sample", "name", "taxid", "reads", "rank", "proportion"])
        frames.append(sdf)
        continue
    sdf = pd.DataFrame(
            data,
        )
    sdf["sample"] = sample
    frames.append(sdf)

df = pd.concat([frame for frame in frames if not frame.empty])
df = df. groupby(["sample", "rank"])[["reads", "proportion"]].sum().reset_index()
df["proportion"] = df["proportion"]*100

fig = px.bar(df, x="reads", y= "sample", color="rank", custom_data="rank",
    labels={"reads": "Read number", "sample": ""},
    orientation='h', template="simple_white")
fig.update_layout(legend={'title_text':''}, hovermode="y unified")
fig.update_yaxes(showspikes=False)
fig.update_traces(hovertemplate="%{customdata}: %{x} reads<extra></extra>")
fig.show()
```

### Percentages

```{python}
#| label: ass-perc

fig = px.bar(df, x="proportion", y= "sample", color="rank", custom_data="rank",
    labels={"prop": "Read proportion (%)", "sample": ""},
    orientation='h', template="simple_white")
fig.update_layout(legend={'title_text':''}, hovermode="y unified")
fig.update_yaxes(showspikes=False)
fig.update_traces(hovertemplate="%{customdata}: %{x}%<extra></extra>")
fig.show()
```

:::

# Software versions

Collected at run time from the software output.

```{python}
#| label: versions

full = pd.DataFrame()

for json_file in json_files:
    with open(json_file) as f:
        jdata = json.load(f)
    sample = jdata["sample"]
    data = jdata["versions"]
    df_data = pd.json_normalize(data)
    index = pd.MultiIndex.from_arrays(zip(*[n.split(".") for n in df_data.columns]))
    df = pd.DataFrame(df_data.loc[0].values, index=index, columns=["versions"])
    if not full.empty:
        full = pd.merge(full, df, how='outer', left_index=True, right_index=True)
        # use max to fill NaN
        full = full.apply(max, axis=1).to_frame()
    else:
        full = df

full = full.reset_index().set_axis(["Module", "Software", "Version"], axis=1)
show(
    full,
    classes="display compact",
    scrollY="600px",
    scrollCollapse=True,
    paging=False,
    buttons=["copyHtml5", "csvHtml5"]
)
```
